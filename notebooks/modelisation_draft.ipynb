{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available:  16\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "import math\n",
    "import pathlib\n",
    "import typing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import albumentations as A\n",
    "from functools import cached_property\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from utils_and_constants import labels, TARGET_SIZE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"NO_ALBUMENTATIONS_UPDATES\"] = \"1\"\n",
    "\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "\n",
    "print(\"Number of CPU cores available: \", NUM_WORKERS := os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Dataset generator for image segmentation tasks.\n",
    "\n",
    "    This class extends the PyDataset utility from TensorFlow/Keras and provides data\n",
    "    loading, optional augmentation, and preprocessing for both images and corresponding segmentation masks.\n",
    "\n",
    "    Attributes:\n",
    "        image_paths (List[pathlib.Path]): List of image file paths.\n",
    "        mask_paths (List[pathlib.Path]): List of corresponding mask file paths.\n",
    "        table_id2category (dict): Mapping from label id to category id.\n",
    "        table_category2name (dict): Mapping from category id to category name.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        target_size (Tuple[int, int]): Target dimensions (height, width) for resizing images and masks.\n",
    "        augmentations (bool): Flag to determine if augmentation pipeline should be applied.\n",
    "        normalize (bool): Flag to normalize image values to [0, 1].\n",
    "        shuffle (bool): Whether to shuffle the dataset at the end of every epoch.\n",
    "        label_onehot (bool): If True, masks will be converted to one-hot encoded format.\n",
    "        sample_weights (List[float] | None): Optional list of weights per mask label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        paths: typing.List[typing.Tuple[pathlib.Path, pathlib.Path]],\n",
    "        labels: typing.NamedTuple,\n",
    "        batch_size: int,\n",
    "        target_size: typing.Tuple[int, int],\n",
    "        augmentations: bool = False,\n",
    "        preview: int = None,  # Load only a subset of the dataset for preview.\n",
    "        normalize: bool | str = True,\n",
    "        shuffle: bool = True,\n",
    "        label_onehot: bool = False,\n",
    "        sample_weights: typing.List[float] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ImageSegmentationDataset.\n",
    "\n",
    "        Args:\n",
    "            paths (List[Tuple[pathlib.Path, pathlib.Path]]): List of tuples containing image and mask paths.\n",
    "            labels (typing.NamedTuple): Named tuple with information for mapping labels (e.g., id, categoryId, category).\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            target_size (Tuple[int, int]): Target size for resizing images and masks (height, width).\n",
    "            augmentations (bool, optional): Apply augmentations if True. Defaults to False.\n",
    "            preview (int, optional): If provided, limits the dataset to the first 'preview' samples. Defaults to None.\n",
    "            normalize (bool, optional): Normalize images by scaling to [0, 1] if True. Defaults to True.\n",
    "            shuffle (bool, optional): Shuffle dataset on each epoch end. Defaults to True.\n",
    "            label_onehot (bool, optional): Convert masks to one-hot encoding if True. Defaults to False.\n",
    "            sample_weights (List[float], optional): List of weights for mask labels. Defaults to None.\n",
    "            **kwargs: Additional keyword arguments for parent class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Load the image and mask paths (apply preview slicing if needed)\n",
    "        self.image_paths, self.mask_paths = self.load_img_and_mask_paths(paths, preview)\n",
    "\n",
    "        # Create mapping tables from labels for category conversions\n",
    "        self.table_id2category = {label.id: label.categoryId for label in labels}\n",
    "        self.table_category2name = {\n",
    "            label.categoryId: label.category for label in labels\n",
    "        }\n",
    "\n",
    "        # Initialize dataset parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.augmentations = augmentations\n",
    "        self.normalize = normalize\n",
    "        self.shuffle = shuffle\n",
    "        self.label_onehot = label_onehot\n",
    "        self.sample_weights = sample_weights\n",
    "\n",
    "        # Initialize augmentation pipeline if enabled\n",
    "        if self.augmentations:\n",
    "            self.compose = A.Compose(\n",
    "                [\n",
    "                    # Geometric transformation: Horizontal flip with probability 0.5.\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    # Color adjustments with one of the provided transformations.\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.RandomBrightnessContrast(\n",
    "                                brightness_limit=0.2, contrast_limit=0.2, p=1.0\n",
    "                            ),\n",
    "                            A.HueSaturationValue(\n",
    "                                hue_shift_limit=10,\n",
    "                                sat_shift_limit=15,\n",
    "                                val_shift_limit=10,\n",
    "                                p=1.0,\n",
    "                            ),\n",
    "                        ],\n",
    "                        p=0.5,\n",
    "                    ),\n",
    "                    # Apply blur or distortion effects.\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "                            A.MotionBlur(blur_limit=5, p=1.0),\n",
    "                            A.OpticalDistortion(distort_limit=0.05, p=1.0),\n",
    "                        ],\n",
    "                        p=0.25,\n",
    "                    ),\n",
    "                ],\n",
    "                # If sample_weights are provided, augment them together with the masks.\n",
    "                additional_targets={\"sample_weights\": \"mask\"}\n",
    "                if self.sample_weights is not None\n",
    "                else {},\n",
    "            )\n",
    "\n",
    "        # Shuffle dataset initially if shuffle is enabled.\n",
    "        if self.shuffle:\n",
    "            self.on_epoch_end()\n",
    "\n",
    "    @cached_property\n",
    "    def num_classes(self):\n",
    "        \"\"\"\n",
    "        Return the number of unique classes in the dataset.\n",
    "        The result is cached after the first calculation.\n",
    "        \"\"\"\n",
    "        return len(set(self.table_id2category.values()))\n",
    "\n",
    "    @cached_property\n",
    "    def num_samples(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        The result is cached after the first calculation.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_img_and_mask_paths(\n",
    "        paths: typing.List[typing.Tuple[pathlib.Path, pathlib.Path]], preview: int\n",
    "    ) -> typing.Tuple[typing.List[pathlib.Path], typing.List[pathlib.Path]]:\n",
    "        \"\"\"\n",
    "        Unpack tuples of image and mask paths and apply preview slicing if specified.\n",
    "\n",
    "        Args:\n",
    "            paths (List[Tuple[pathlib.Path, pathlib.Path]]): List of tuples where each tuple contains\n",
    "                (image_path, mask_path).\n",
    "            preview (int): If provided, only load the first 'preview' entries from the lists.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[pathlib.Path], List[pathlib.Path]]: Two lists containing image and mask paths.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the number of images and masks do not match.\n",
    "        \"\"\"\n",
    "        # Unpack the tuples into separate lists\n",
    "        image_paths, mask_paths = zip(*paths)\n",
    "        if len(image_paths) != len(mask_paths):\n",
    "            raise ValueError(\"Number of images and masks must be equal.\")\n",
    "        # Slice lists if preview limit is set\n",
    "        if preview is not None:\n",
    "            image_paths = image_paths[:preview]\n",
    "            mask_paths = mask_paths[:preview]\n",
    "        return list(image_paths), list(mask_paths)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of batches per epoch.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of batches.\n",
    "        \"\"\"\n",
    "        length = math.ceil(self.num_samples / self.batch_size)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Fetch a batch of images and masks at the given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray] or Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "                Depending on whether sample_weights is provided; returns images, masks and optionally weights.\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If the calculated start index is out of range.\n",
    "        \"\"\"\n",
    "        # Determine start and end indices for this batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min(start_idx + self.batch_size, self.num_samples)\n",
    "        if start_idx >= self.num_samples:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        # Pair image and mask paths for the current batch\n",
    "        batch_paths = list(\n",
    "            zip(self.image_paths[start_idx:end_idx], self.mask_paths[start_idx:end_idx])\n",
    "        )\n",
    "\n",
    "        # Process each pair: load and apply augmentations (if enabled)\n",
    "        results = [self.load_and_augment(pair) for pair in batch_paths]\n",
    "\n",
    "        # Unpack and return batch data based on sample weight availability\n",
    "        if self.sample_weights is not None:\n",
    "            images, masks, weights = zip(*results)\n",
    "            return np.asarray(images), np.asarray(masks), np.asarray(weights)\n",
    "        else:\n",
    "            images, masks = zip(*results)\n",
    "            return np.asarray(images), np.asarray(masks)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle the dataset at the end of each epoch if shuffling is enabled.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            # Combine paths into list of tuples, shuffle and then unpack.\n",
    "            zip_paths = list(zip(self.image_paths, self.mask_paths))\n",
    "            np.random.shuffle(zip_paths)\n",
    "            self.image_paths, self.mask_paths = zip(*zip_paths)\n",
    "        # If shuffle is disabled, do nothing.\n",
    "\n",
    "    def _normalize_img(self, img_array):\n",
    "        \"\"\"\n",
    "        Normalize an image array to a range of [0, 1].\n",
    "\n",
    "        Args:\n",
    "            img_array (np.ndarray): Image data as a numpy array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Normalized image array.\n",
    "        \"\"\"\n",
    "        match self.normalize:\n",
    "            case True:\n",
    "                return img_array / 255.0\n",
    "            case False:\n",
    "                return img_array\n",
    "            case _:\n",
    "                try:\n",
    "                    preprocess_input = sm.get_preprocessing(self.normalize)\n",
    "                    return preprocess_input(img_array)\n",
    "                except AttributeError as e:\n",
    "                    print(\n",
    "                        f\"Invalid normalization option: {self.normalize}. Should be True, False or a valid backbone name.\"\n",
    "                    )\n",
    "                    raise e\n",
    "\n",
    "    def load_img_to_array(self, img_path: pathlib.Path):\n",
    "        \"\"\"\n",
    "        Load an image from disk, resize, and convert it to a numpy array.\n",
    "\n",
    "        Args:\n",
    "            img_path (pathlib.Path): File path to the image.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Image data as a numpy array (optionally normalized).\n",
    "        \"\"\"\n",
    "        img = tf.keras.utils.load_img(\n",
    "            str(img_path),\n",
    "            target_size=self.target_size,\n",
    "            color_mode=\"rgb\",\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "        img_array = tf.keras.utils.img_to_array(img, dtype=np.float32)\n",
    "        if self.normalize:\n",
    "            img_array = self._normalize_img(img_array)\n",
    "        return img_array\n",
    "\n",
    "    def load_mask_to_array(self, mask_path: pathlib.Path):\n",
    "        \"\"\"\n",
    "        Load a mask from disk, resize, map mask ids to category ids, and optionally one-hot encode.\n",
    "\n",
    "        Args:\n",
    "            mask_path (pathlib.Path): File path to the mask image.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Processed mask array.\n",
    "        \"\"\"\n",
    "        mask = tf.keras.utils.load_img(\n",
    "            str(mask_path),\n",
    "            target_size=self.target_size,\n",
    "            color_mode=\"grayscale\",\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "        mask_array = tf.keras.utils.img_to_array(mask, dtype=np.int8)\n",
    "        # Map mask ids using the table; squeeze is used to drop extra dimensions.\n",
    "        mask_array = np.vectorize(self.table_id2category.get)(mask_array).squeeze()\n",
    "        if self.label_onehot:\n",
    "            mask_array = tf.keras.utils.to_categorical(\n",
    "                mask_array, num_classes=self.num_classes\n",
    "            )\n",
    "        return mask_array\n",
    "\n",
    "    def load_and_augment(self, paths):\n",
    "        \"\"\"\n",
    "        Load image and mask, and apply augmentations if enabled.\n",
    "\n",
    "        Args:\n",
    "            paths (Tuple[pathlib.Path, pathlib.Path]): Tuple containing image and mask paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray] or Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "                Processed image, mask and optionally sample weights.\n",
    "        \"\"\"\n",
    "        img_path, mask_path = paths\n",
    "        img = self.load_img_to_array(img_path)\n",
    "        mask = self.load_mask_to_array(mask_path)\n",
    "\n",
    "        if self.sample_weights is not None:\n",
    "            weights = np.take(self.sample_weights, mask)\n",
    "            if self.augmentations:\n",
    "                augmented = self.compose(image=img, mask=mask, sample_weights=mask)\n",
    "                return (\n",
    "                    augmented[\"image\"],\n",
    "                    augmented[\"mask\"],\n",
    "                    augmented[\"sample_weights\"],\n",
    "                )\n",
    "            else:\n",
    "                return img, mask, weights\n",
    "        else:\n",
    "            if self.augmentations:\n",
    "                augmented = self.compose(image=img, mask=mask)\n",
    "                return augmented[\"image\"], augmented[\"mask\"]\n",
    "            else:\n",
    "                return img, mask\n",
    "\n",
    "    def get_image_and_mask(self, index: int):\n",
    "        \"\"\"\n",
    "        Retrieve a single image/mask pair for visualization purposes.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, Tuple[pathlib.Path, pathlib.Path]]:\n",
    "                Processed image, mask and original file paths.\n",
    "        \"\"\"\n",
    "        paths = (self.image_paths[index], self.mask_paths[index])\n",
    "        if self.sample_weights is None:\n",
    "            img, mask = self.load_and_augment(paths)\n",
    "        else:\n",
    "            img, mask, _ = self.load_and_augment(paths)\n",
    "        return img, mask, paths\n",
    "\n",
    "    def get_prediction(self, model, index: int):\n",
    "        \"\"\"\n",
    "        Generate a prediction for a given image sample using the provided model.\n",
    "\n",
    "        Args:\n",
    "            model: A trained segmentation model.\n",
    "            index (int): Index of the sample to predict.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted mask with labels.\n",
    "        \"\"\"\n",
    "        img, _, _ = self.get_image_and_mask(index)\n",
    "        mask_pred = model.predict(np.expand_dims(img, axis=0))\n",
    "        mask_pred = np.argmax(mask_pred.squeeze(), axis=-1)\n",
    "        return mask_pred\n",
    "\n",
    "    def show_transformation(self, index: int, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Display the original and transformed images and masks for a given sample.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "            figsize (tuple, optional): Figure size for the plot. Defaults to (10, 6).\n",
    "        \"\"\"\n",
    "        # Retrieve sample data and file paths\n",
    "        img, mask, paths = self.get_image_and_mask(index)\n",
    "        img_path, mask_path = paths\n",
    "\n",
    "        # Prepare subplots for original and transformed images/masks\n",
    "        fig, ax = plt.subplots(2, 2, figsize=figsize)\n",
    "        ax[0, 0].imshow(plt.imread(img_path))\n",
    "        ax[0, 0].set_title(\"Original Image\")\n",
    "        ax[0, 1].imshow(img)\n",
    "        ax[0, 1].set_title(\"Transformed Image\")\n",
    "        ax[1, 0].imshow(plt.imread(mask_path))\n",
    "        ax[1, 0].set_title(\"Original Mask\")\n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title(\"Transformed Mask\")\n",
    "        plt.show()\n",
    "\n",
    "    def show_prediction(self, model, index: int, figsize=(15, 6)):\n",
    "        \"\"\"\n",
    "        Display the original image, ground truth, and model prediction for a sample.\n",
    "\n",
    "        Args:\n",
    "            model: A trained segmentation model.\n",
    "            index (int): Index of the sample.\n",
    "            figsize (tuple, optional): Figure size for the plot. Defaults to (15, 6).\n",
    "        \"\"\"\n",
    "        # Retrieve processed sample and generate prediction\n",
    "        img, mask, _ = self.get_image_and_mask(index)\n",
    "        mask_pred = self.get_prediction(model, index)\n",
    "\n",
    "        # Initialize figure with a constrained layout\n",
    "        fig = plt.figure(layout=\"constrained\", figsize=figsize)\n",
    "        fig.suptitle(f\"{model.name} Predictions\", fontsize=16)\n",
    "\n",
    "        # Create top and bottom subfigures for comparing ground truth and predictions\n",
    "        subfigs = fig.subfigures(2, 1, wspace=0.07)\n",
    "        axsTop = subfigs[0].subplots(1, 3, sharey=True, sharex=True)\n",
    "        axsBottom = subfigs[1].subplots(1, 3, sharey=True, sharex=True)\n",
    "\n",
    "        # Plot original image, ground truth mask (grayscale and RGB)\n",
    "        for i, ax in enumerate(axsTop):\n",
    "            if i == 0:\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(\"Original Image\")\n",
    "            elif i == 1:\n",
    "                ax.imshow(mask, cmap=\"Greys\")\n",
    "                ax.set_title(\"Ground Truth Mask (Grayscale)\")\n",
    "            else:\n",
    "                ax.imshow(mask)\n",
    "                ax.set_title(\"Ground Truth Mask (RGB)\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Plot original image, predicted mask (grayscale and RGB)\n",
    "        for i, ax in enumerate(axsBottom):\n",
    "            if i == 0:\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(\"Original Image\")\n",
    "            elif i == 1:\n",
    "                ax.imshow(mask_pred, cmap=\"Greys\")\n",
    "                ax.set_title(\"Predicted Mask (Grayscale)\")\n",
    "            else:\n",
    "                ax.imshow(mask_pred)\n",
    "                ax.set_title(\"Predicted Mask (RGB)\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passons à la préparation des variables pour charger les répertoires d'entraînement (split en train et val) et de test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files:  8925\n",
      "Test files:  1500\n"
     ]
    }
   ],
   "source": [
    "# Path to the root directory\n",
    "root_dir = pathlib.Path.cwd().parent\n",
    "# Path to the train directory\n",
    "train_dir = root_dir / \"data\" / \"Sorted\" / \"train\"\n",
    "# Path to the test directory\n",
    "test_dir = root_dir / \"data\" / \"Sorted\" / \"test\"\n",
    "# Print the number of files after sorting the files\n",
    "print(\"Train files: \", len(os.listdir(train_dir)))\n",
    "print(\"Test files: \", len(os.listdir(test_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train paths:  2380\n",
      "Val paths:  595\n",
      "Test paths:  500\n"
     ]
    }
   ],
   "source": [
    "# Get the full train image and masks paths to analyse the classes\n",
    "image_paths = sorted(list(train_dir.glob(\"*leftImg8bit.png\")))\n",
    "mask_paths = sorted(list(train_dir.glob(\"*labelIds.png\")))\n",
    "full_train_paths = list(zip(image_paths, mask_paths))\n",
    "\n",
    "# Split the full train paths into train and validation sets\n",
    "image_train, image_val, mask_train, mask_val = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.2, random_state=314\n",
    ")\n",
    "train_paths = list(zip(image_train, mask_train))\n",
    "val_paths = list(zip(image_val, mask_val))\n",
    "\n",
    "# Finally, get the test image and mask paths\n",
    "image_paths = sorted(list(test_dir.glob(\"*leftImg8bit.png\")))\n",
    "mask_paths = sorted(list(test_dir.glob(\"*labelIds.png\")))\n",
    "test_paths = list(zip(image_paths, mask_paths))\n",
    "\n",
    "# Print len of train, val and test paths\n",
    "print(\"Train paths: \", len(train_paths))\n",
    "print(\"Val paths: \", len(val_paths))\n",
    "print(\"Test paths: \", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, smooth=1e-5, name=\"dice_metric\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth = smooth\n",
    "        self.total_dice = self.add_weight(name=\"total_dice\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert the sparse y_true to one-hot encoding\n",
    "        y_true_onehot = tf.one_hot(\n",
    "            y_true, depth=self.num_classes\n",
    "        )  # (batch, h, w, num_classes)\n",
    "\n",
    "        # Convert the y_pred probabilities with argmax\n",
    "        y_pred_indices = tf.argmax(y_pred, axis=-1)  # (batch, h, w)\n",
    "        # Convert the y_pred indices to one-hot encoding\n",
    "        y_pred_onehot = tf.one_hot(\n",
    "            y_pred_indices, depth=self.num_classes\n",
    "        )  # (batch, h, w, num_classes)\n",
    "\n",
    "        # Flatten the one-hot tensors on all axes except batch\n",
    "        y_true_flat = tf.reshape(\n",
    "            y_true_onehot, [tf.shape(y_true_onehot)[0], -1, self.num_classes]\n",
    "        )  # (batch, h*w, num_classes)\n",
    "        y_pred_flat = tf.reshape(\n",
    "            y_pred_onehot, [tf.shape(y_pred_onehot)[0], -1, self.num_classes]\n",
    "        )  # (batch, h*w, num_classes)\n",
    "\n",
    "        # Compute intersection and union for each class\n",
    "        intersection = tf.reduce_sum(\n",
    "            y_true_flat * y_pred_flat, axis=1\n",
    "        )  # (batch, num_classes)\n",
    "        union = tf.reduce_sum(y_true_flat, axis=1) + tf.reduce_sum(\n",
    "            y_pred_flat, axis=1\n",
    "        )  # (batch, num_classes)\n",
    "\n",
    "        # Compute DICE score for each class with the smoothening factor\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        # Compute the DICE mean score for each image\n",
    "        dice_per_image = tf.reduce_mean(dice, axis=-1)  # (batch,)\n",
    "\n",
    "        # Update the total dice and count with the mean dice score per image\n",
    "        self.total_dice.assign_add(tf.reduce_sum(dice_per_image))\n",
    "        self.count.assign_add(tf.cast(tf.size(dice_per_image), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Compute the mean dice score over all images\n",
    "        return self.total_dice / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset the total dice and count at the end of each epoch\n",
    "        self.total_dice.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://segmentation-models.readthedocs.io/en/latest/api.html#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = sm.metrics.IOUScore(threshold=0.5)\n",
    "dicecoef = sm.metrics.FScore(beta=1.0, threshold=0.5)\n",
    "sm.metrics.dice_score = DiceMetric(num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MeanIoU.__init__() got an unexpected keyword argument 'sparse_y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mean_metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMeanIoU\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeanIoU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_y_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# As we are using sparse categorical crossentropy, the ground truth is sparse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_y_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# As we are using softmax activation, the output is not sparse\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     10\u001b[0m     DiceMetric(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDice\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPixel_Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/github/oc_p8/.venv/lib/python3.10/site-packages/keras/dtensor/utils.py:141\u001b[0m, in \u001b[0;36minject_mesh.<locals>._wrap_function\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m   instance\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;241m=\u001b[39m mesh  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: MeanIoU.__init__() got an unexpected keyword argument 'sparse_y_true'"
     ]
    }
   ],
   "source": [
    "mean_metrics = [\n",
    "    tf.keras.metrics.MeanIoU(\n",
    "        num_classes=8,\n",
    "        name=\"MeanIoU\",\n",
    "        dtype=None,\n",
    "        sparse_y_true=True,  # As we are using sparse categorical crossentropy, the ground truth is sparse\n",
    "        sparse_y_pred=False,  # As we are using softmax activation, the output is not sparse\n",
    "        axis=-1,\n",
    "    ),\n",
    "    DiceMetric(num_classes=8, name=\"Dice\"),\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"Pixel_Accuracy\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_metrics = [\n",
    "    tf.keras.metrics.IoU(\n",
    "        num_classes=8,\n",
    "        target_class_ids=[i],  # Compute the IoU for the background class (class 0)\n",
    "        name=f\"IoU_class_{i}\",\n",
    "        sparse_y_true=True,\n",
    "        sparse_y_pred=False,\n",
    "        axis=-1,\n",
    "    )\n",
    "    for i in range(8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetric:\n",
    "    def __init__(\n",
    "        self,\n",
    "        history: callbacks.history.History,\n",
    "        results: dict,\n",
    "        extra_metrics: dict,\n",
    "        monitor: str,\n",
    "        best_monitor: str,\n",
    "    ):\n",
    "        \"\"\" \"\"\"\n",
    "        self.history_metrics = {\n",
    "            (f\"train_{k}\" if not k.startswith(\"val\") else k): v\n",
    "            for k, v in history.history.items()\n",
    "        }\n",
    "        self.history_params = history.params\n",
    "        self.epochs_list = history.epoch\n",
    "        self.model_name = history._model.name\n",
    "        self.results = results\n",
    "        self.extra_metrics = extra_metrics\n",
    "        self.monitor = monitor\n",
    "        self.best_monitor = best_monitor\n",
    "\n",
    "    @classmethod\n",
    "    def from_pickle(\n",
    "        cls,\n",
    "        pickle_path: str,\n",
    "    ):\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @property\n",
    "    def best_epoch(self):\n",
    "        # Take the epoch with the best validation loss\n",
    "        match self.best_monitor.lower().strip():\n",
    "            case \"max\":\n",
    "                return self.history_metrics[self.monitor].index(\n",
    "                    max(self.history_metrics[self.monitor])\n",
    "                )\n",
    "            case \"min\":\n",
    "                return self.history_metrics[self.monitor].index(\n",
    "                    min(self.history_metrics[self.monitor])\n",
    "                )\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown monitor: {self.monitor}. Must be 'max' or 'min'.\"\n",
    "                )\n",
    "\n",
    "    @cached_property\n",
    "    def metrics_df(self):\n",
    "        # Create a DataFrame with the best epoch metrics\n",
    "        train_val_metrics = (\n",
    "            pd.DataFrame(self.history_metrics)\n",
    "            .iloc[self.best_epoch, :]\n",
    "            .to_frame()\n",
    "            .T.reset_index(drop=True)\n",
    "        )\n",
    "        test_metrics = pd.DataFrame(\n",
    "            data=[self.results.values()],\n",
    "            columns=[f\"test_{k}\" for k in self.results.keys()],\n",
    "        )\n",
    "        metrics_df = pd.concat([train_val_metrics, test_metrics], axis=1)\n",
    "        metrics_df = metrics_df.assign(\n",
    "            best_epoch=self.best_epoch,\n",
    "            n_epochs=self.history_params.get(\"epochs\"),\n",
    "            n_steps=self.history_params.get(\"steps\"),\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "        metrics_df.insert(\n",
    "            0, \"experiment_folder\", self.extra_metrics.get(\"experiment_folder\")\n",
    "        )\n",
    "        for key, value in self.extra_metrics.items():\n",
    "            if key != \"experiment_folder\":\n",
    "                metrics_df[key] = value\n",
    "        return metrics_df\n",
    "\n",
    "    def save_metrics(\n",
    "        self, metrics_path: str = \"../metrics.csv\", artifacts_path: str = \"../artifacts\"\n",
    "    ):\n",
    "        # Save metrics as a CSV file if it does not exist\n",
    "        if not os.path.exists(metrics_path):\n",
    "            self.metrics_df.to_csv(metrics_path, mode=\"a\", header=True, index=False)\n",
    "            print(f\"Metrics file created at {metrics_path}\")\n",
    "        # Else, append the metrics to the existing file\n",
    "        else:\n",
    "            self.metrics_df.to_csv(metrics_path, mode=\"a\", header=False, index=False)\n",
    "            print(f\"Metrics file updated at {metrics_path}\")\n",
    "\n",
    "        # Save model_metric as a pickle file\n",
    "        with open(\n",
    "            f\"{artifacts_path}/{self.extra_metrics.get('experiment_folder')}/model_metric.pkl\",\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(self, f)\n",
    "            print(f\"Pickle file created at {artifacts_path}\")\n",
    "\n",
    "    def plot_loss(self, figsize=(12, 6), artifacts_path: str = \"../artifacts\"):\n",
    "        fig_path = f\"{artifacts_path}/{self.extra_metrics.get('experiment_folder')}/plot_loss.png\"\n",
    "        # Plot the training and validation loss\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.plot(\n",
    "            self.epochs_list,\n",
    "            self.history_metrics[\"train_loss\"],\n",
    "            label=\"train_loss\",\n",
    "            color=\"blue\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            self.epochs_list,\n",
    "            self.history_metrics[\"val_loss\"],\n",
    "            label=\"val_loss\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{self.model_name} Training and Validation Loss Curve per epoch\")\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig.savefig(fig_path)\n",
    "\n",
    "    def plot_train_metrics(\n",
    "        self, figsize=(20, 10), artifacts_path: str = \"../artifacts\"\n",
    "    ):\n",
    "        fig_path = f\"{artifacts_path}/{self.extra_metrics.get('experiment_folder')}/plot_train_metrics.png\"\n",
    "        history_metrics_df = pd.DataFrame(self.history_metrics)\n",
    "        fig, ax = plt.subplots(\n",
    "            2, 2, figsize=figsize, sharex=True, sharey=True, tight_layout=True\n",
    "        )\n",
    "        fig.suptitle(f\"{self.model_name} Training Metrics Curve per epoch\", fontsize=16)\n",
    "        for i, split in enumerate((\"train\", \"val\")):\n",
    "            history_metrics_df.filter(\n",
    "                [\n",
    "                    col\n",
    "                    for col in self.history_metrics.keys()\n",
    "                    if col.startswith(f\"{split}\")\n",
    "                    and \"loss\" not in col\n",
    "                    and \"class\" not in col\n",
    "                ]\n",
    "            ).plot(ax=ax[0, i], title=f\"{split} mean metrics\", grid=True).legend(\n",
    "                bbox_to_anchor=(1.0, 1.0)\n",
    "            )\n",
    "        for i, split in enumerate((\"train\", \"val\")):\n",
    "            history_metrics_df.filter(\n",
    "                [\n",
    "                    col\n",
    "                    for col in self.history_metrics.keys()\n",
    "                    if col.startswith(f\"{split}_IoU\") and \"loss\" not in col\n",
    "                ]\n",
    "            ).plot(ax=ax[1, i], title=f\"{split} per class metrics\", grid=True).legend(\n",
    "                bbox_to_anchor=(1.0, 1.0)\n",
    "            )\n",
    "        plt.show()\n",
    "        fig.savefig(fig_path)\n",
    "\n",
    "    def plot_test_metrics(self, figsize=(20, 10), artifacts_path: str = \"../artifacts\"):\n",
    "        # fig_path = f\"{artifacts_path}/{self.extra_metrics.get('experiment_folder')}/plot_test_metrics.png\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparativeMetric:\n",
    "    def __init__(self, path=\"../metrics.csv\"):\n",
    "        self.metrics_df = pd.read_csv(path).drop_duplicates(keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, num_classes, from_logits=False, smooth=1e-6, name=\"dice_loss\"):\n",
    "        \"\"\"\n",
    "        Dice Loss pour segmentation multi-classe.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Nombre de classes dans la segmentation.\n",
    "            from_logits (bool): Si True, applique `softmax` avant le calcul.\n",
    "            smooth (float): Facteur de lissage pour éviter la division par zéro.\n",
    "            name (str): Nom de la loss.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.from_logits = from_logits\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcul de la perte Dice.\n",
    "\n",
    "        Args:\n",
    "            y_true (tensor): Vraies étiquettes (batch, H, W) ou (batch, H, W, C).\n",
    "            y_pred (tensor): Prédictions (batch, H, W, C).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Dice Loss\n",
    "        \"\"\"\n",
    "        # Convertir y_true en one-hot si nécessaire\n",
    "        if tf.shape(y_true).shape == 3:  # (batch, H, W) → (batch, H, W, C)\n",
    "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes)\n",
    "\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.softmax(\n",
    "                y_pred, axis=-1\n",
    "            )  # Conversion des logits en probabilités\n",
    "\n",
    "        # Calcul de l'intersection et de l'union pour chaque classe\n",
    "        intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "\n",
    "        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        # Moyenne sur toutes les classes\n",
    "        return 1 - tf.reduce_mean(dice_score, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class from tf.keras.losses.Loss combining dice loss and sparse categorical crossentropy\n",
    "class DiceCrossEntropyLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        from_logits=False,\n",
    "        smooth=1e-6,\n",
    "        alpha=0.7,\n",
    "        name=\"dice_crossentropy_loss\",\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.from_logits = from_logits\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the dice loss\n",
    "        dice_loss = DiceLoss(\n",
    "            num_classes=self.num_classes,\n",
    "            from_logits=self.from_logits,\n",
    "            smooth=self.smooth,\n",
    "        )(y_true, y_pred)\n",
    "        # Compute the sparse categorical crossentropy loss\n",
    "        scce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=self.from_logits\n",
    "        )(y_true, y_pred)\n",
    "        # Return the sum of both losses weighted by alpha\n",
    "        return self.alpha * dice_loss + (1 - self.alpha) * scce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class from tf.keras.losses.Loss combining dice loss and focal loss\n",
    "class DiceFocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        from_logits=False,\n",
    "        smooth=1e-6,\n",
    "        alpha=0.7,\n",
    "        gamma=2.0,\n",
    "        name=\"dice_focal_loss\",\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.from_logits = from_logits\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the dice loss\n",
    "        dice_loss = DiceLoss(\n",
    "            num_classes=self.num_classes,\n",
    "            from_logits=self.from_logits,\n",
    "            smooth=self.smooth,\n",
    "        )(y_true, y_pred)\n",
    "        # Compute the focal loss\n",
    "        focal_loss = SparseCategoricalFocalLoss(\n",
    "            gamma=self.gamma,\n",
    "            from_logits=self.from_logits,\n",
    "            name=\"sparse_categorical_focal_loss\",\n",
    "        )(y_true, y_pred)\n",
    "        # Return the sum of both losses\n",
    "        return self.alpha * dice_loss + (1 - self.alpha) * focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Advanced Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. UNET DECODER + ENCODER RESNET (+imagepreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = \"resnet50\"\n",
    "\n",
    "train_datagen = ImageSegmentationDataset(\n",
    "    paths=train_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=True,\n",
    "    shuffle=True,\n",
    "    normalize=BACKBONE,\n",
    "    preview=128,\n",
    ")\n",
    "\n",
    "val_datagen = ImageSegmentationDataset(\n",
    "    paths=val_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=False,\n",
    "    shuffle=False,\n",
    "    normalize=BACKBONE,\n",
    "    preview=32,\n",
    ")\n",
    "\n",
    "test_datagen = ImageSegmentationDataset(\n",
    "    paths=test_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=False,\n",
    "    shuffle=False,\n",
    "    normalize=BACKBONE,\n",
    "    preview=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet(\n",
    "    BACKBONE,\n",
    "    classes=train_datagen.num_classes,\n",
    "    activation=\"softmax\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    input_shape=(256, 512, 3),\n",
    "    encoder_freeze=True,\n",
    ")\n",
    "\n",
    "# Folder to create to store artifacts\n",
    "new_folder = 610\n",
    "\n",
    "# Create a model checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"../artifacts/{new_folder}/best_model.keras\",  # Save under keras format to avoid WARNING\n",
    "    monitor=\"val_MeanIoU\",  # Set Mean IoU as the metric to monitor to save the best model\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Create an early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_MeanIoU\",  # Set Mean IoU as the metric to monitor to stop the training\n",
    "    patience=4,  # Epochs to wait before stopping\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create a ReduceLROnPlateau callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_MeanIoU\",\n",
    "    factor=0.5,  # Factor to reduce the learning rate\n",
    "    patience=2,  # Epochs to wait before reducing the learning rate\n",
    "    mode=\"max\",\n",
    "    min_lr=1e-6,  # Minimum learning rate\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = DiceFocalLoss(\n",
    "    num_classes=train_datagen.num_classes,\n",
    "    from_logits=False,\n",
    "    smooth=1e-6,\n",
    "    alpha=0.8,\n",
    "    name=\"dice_focal_loss\",\n",
    ")\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4)\n",
    "\n",
    "# Pretrain the model decoder for the 2 first epochs\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=mean_metrics + per_class_metrics,\n",
    ")\n",
    "\n",
    "# Pretrain the model decoder for the 2 first epochs\n",
    "history_pretrain = model.fit_generator(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=2,\n",
    "    workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "sm.utils.set_trainable(model)\n",
    "\n",
    "# Train the full model for 50 epochs\n",
    "history_train = model.fit_generator(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=4,\n",
    "    workers=NUM_WORKERS,\n",
    "    callbacks=[checkpoint_callback, early_stopping, reduce_lr],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to create full history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    \"unet_resnet_model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharge les meilleurs poids enregistrés lors de l'entraînement\n",
    "model.load_weights(f\"../artifacts/{new_folder}/best_model.keras\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "results = model.evaluate(test_datagen, return_dict=True)\n",
    "\n",
    "# Get a fixed test img for computing inference time and for further visualization\n",
    "start_time = time.time()\n",
    "img_preds = test_datagen.get_prediction(model, 11)\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time on a unique prediction: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_metric = ModelMetric.from_pickle(\n",
    "        pickle_path=\"../artifacts/61/model_metric.pkl\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    # Set the extra_metrics to save in the ModelMetric object\n",
    "    extra_metrics = {\n",
    "        \"experiment_folder\": new_folder,  # experiment_folder is mandatory to save the metrics\n",
    "        \"inference_time\": inference_time,\n",
    "        \"classes\": train_datagen.num_classes,\n",
    "        \"train_samples\": train_datagen.num_samples,\n",
    "        \"val_samples\": val_datagen.num_samples,\n",
    "        \"test_samples\": test_datagen.num_samples,\n",
    "        \"train_batches\": len(train_datagen),\n",
    "        \"augmentations\": str(train_datagen.augmentations),\n",
    "        \"normalize\": str(train_datagen.normalize),\n",
    "        \"loss_fn\": loss_fn.name,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"learning_rate\": float(optimizer.learning_rate.value.numpy()),\n",
    "        # \"weight_decay\": float(optimizer.weight_decay.value.numpy()),\n",
    "    }\n",
    "\n",
    "    # Create the ModelMetric object\n",
    "    model_metric = ModelMetric(\n",
    "        history=history,\n",
    "        results=results,\n",
    "        extra_metrics=extra_metrics,\n",
    "        monitor=\"val_MeanIoU\",\n",
    "        best_monitor=\"max\",\n",
    "    )\n",
    "\n",
    "    # Save the metrics\n",
    "    model_metric.save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. FPN DECODER + EFFICIENT NET ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = \"efficientnetb0\"\n",
    "\n",
    "train_datagen = ImageSegmentationDataset(\n",
    "    paths=train_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=True,\n",
    "    shuffle=True,\n",
    "    normalize=BACKBONE,\n",
    "    preview=128,\n",
    ")\n",
    "\n",
    "val_datagen = ImageSegmentationDataset(\n",
    "    paths=val_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=False,\n",
    "    shuffle=False,\n",
    "    normalize=BACKBONE,\n",
    "    preview=32,\n",
    ")\n",
    "\n",
    "test_datagen = ImageSegmentationDataset(\n",
    "    paths=test_paths,\n",
    "    labels=labels,\n",
    "    batch_size=16,  # We won't use batches here\n",
    "    target_size=TARGET_SIZE,\n",
    "    workers=NUM_WORKERS,\n",
    "    augmentations=False,\n",
    "    shuffle=False,\n",
    "    normalize=BACKBONE,\n",
    "    preview=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.FPN(\n",
    "    BACKBONE,\n",
    "    classes=train_datagen.num_classes,\n",
    "    activation=\"softmax\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    input_shape=(256, 512, 3),\n",
    "    encoder_freeze=True,\n",
    ")\n",
    "\n",
    "# Folder to create to store artifacts\n",
    "new_folder = 611\n",
    "\n",
    "# Create a model checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"../artifacts/{new_folder}/best_model.keras\",  # Save under keras format to avoid WARNING\n",
    "    monitor=\"val_MeanIoU\",  # Set Mean IoU as the metric to monitor to save the best model\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Create an early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_MeanIoU\",  # Set Mean IoU as the metric to monitor to stop the training\n",
    "    patience=4,  # Epochs to wait before stopping\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create a ReduceLROnPlateau callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_MeanIoU\",\n",
    "    factor=0.5,  # Factor to reduce the learning rate\n",
    "    patience=2,  # Epochs to wait before reducing the learning rate\n",
    "    mode=\"max\",\n",
    "    min_lr=1e-6,  # Minimum learning rate\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = DiceFocalLoss(\n",
    "    num_classes=train_datagen.num_classes,\n",
    "    from_logits=False,\n",
    "    smooth=1e-6,\n",
    "    alpha=0.8,\n",
    "    name=\"dice_focal_loss\",\n",
    ")\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4)\n",
    "\n",
    "# Pretrain the model decoder for the 2 first epochs\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=mean_metrics + per_class_metrics,\n",
    ")\n",
    "\n",
    "# Pretrain the model decoder for the 2 first epochs\n",
    "history_pretrain = model.fit_generator(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=2,\n",
    "    workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "sm.utils.set_trainable(model)\n",
    "\n",
    "# Train the full model for 50 epochs\n",
    "history_train = model.fit_generator(\n",
    "    train_datagen,\n",
    "    validation_data=val_datagen,\n",
    "    epochs=4,\n",
    "    workers=NUM_WORKERS,\n",
    "    callbacks=[checkpoint_callback, early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    \"fpn_efficientnet_model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharge les meilleurs poids enregistrés lors de l'entraînement\n",
    "model.load_weights(\"../artifacts/62/best_model.keras\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "results = model.evaluate(test_datagen, return_dict=True)\n",
    "\n",
    "# Get a fixed test img for computing inference time and for further visualization\n",
    "start_time = time.time()\n",
    "img_preds = test_datagen.get_prediction(model, 11)\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time on a unique prediction: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_metric = ModelMetric.from_pickle(\n",
    "        pickle_path=\"../artifacts/62/model_metric.pkl\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    # Set the extra_metrics to save in the ModelMetric object\n",
    "    extra_metrics = {\n",
    "        \"experiment_folder\": new_folder,  # experiment_folder is mandatory to save the metrics\n",
    "        \"inference_time\": inference_time,\n",
    "        \"classes\": train_datagen.num_classes,\n",
    "        \"train_samples\": train_datagen.num_samples,\n",
    "        \"val_samples\": val_datagen.num_samples,\n",
    "        \"test_samples\": test_datagen.num_samples,\n",
    "        \"train_batches\": len(train_datagen),\n",
    "        \"augmentations\": str(train_datagen.augmentations),\n",
    "        \"normalize\": str(train_datagen.normalize),\n",
    "        \"loss_fn\": loss_fn.name,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"learning_rate\": float(optimizer.learning_rate.value.numpy()),\n",
    "        # \"weight_decay\": float(optimizer.weight_decay.value.numpy()),\n",
    "    }\n",
    "\n",
    "    # Create the ModelMetric object\n",
    "    model_metric = ModelMetric(\n",
    "        history=history,\n",
    "        results=results,\n",
    "        extra_metrics=extra_metrics,\n",
    "        monitor=\"val_MeanIoU\",\n",
    "        best_monitor=\"max\",\n",
    "    )\n",
    "\n",
    "    # Save the metrics\n",
    "    model_metric.save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. SOTA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
